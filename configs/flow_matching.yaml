data:
  dataset: "celeba"
  root: "/home/arnavgoe/10799-Diffusion/cmu-10799-diffusion/data" # Your dataset path
  from_hub: false  # run "python download_dataset.py --output_dir YOUR_PATH" first if you want to use false here
  repo_name: "electronickale/cmu-10799-celeba64-subset"
  image_size: 64
  channels: 3
  num_workers: 4
  pin_memory: true
  augment: true

# TODO: build your model architecture, feel free to add or exclude any of these options
model:
  base_channels: 64 # int
  channel_mult: [1, 2, 2, 4] # list of int [1, 2, 2, 4]
  num_res_blocks: 2 # TODO
  attention_resolutions: [16, 8] # list of int
  num_heads: 4 # int
  dropout: 0.0 # float
  use_scale_shift_norm: true # bool

# TODO: your training hyperparameters, feel free to add or exclude any of these options
training:
  batch_size: 128 # int
  learning_rate: 0.0002 # float
  weight_decay: 0.0 # float
  betas: [0.9, 0.999] # list of float
  ema_decay: 0.9999 # float
  ema_start: 1000 # int # This is the start iter to use EMA during sampling at training time, as using EMA at the beginning may not be very meaningful
  gradient_clip_norm: 1.0 # float
  num_iterations: 120000 # int
  log_every: 10 # int
  sample_every: 10000 # int
  save_every: 20000 # int
  num_samples: 16 # int

# TODO: your ddpm algorithm hyperparameters, feel free to add or exclude any of these options
ddpm:
  num_timesteps: 1000   # int
  beta_start: 0.0001 # float
  beta_end: 0.02 # float

flow_matching:
  num_timesteps: 1000  # int

sampling:
  num_steps: 100 # int
  sampler: "flow_matching" # hw2

infrastructure:
  seed: 42
  device: "cuda"
  num_gpus: 1 # int # 1 should be ok
  mixed_precision: true # bool # both true and false should work
  compile_model: false # only tested false

checkpoint:
  dir: "/data/user_data/arnavgoe/cmu-10799-diffusion/checkpoints_v2"
  resume: null

logging:
  dir: "/home/arnavgoe/10799-Diffusion/cmu-10799-diffusion/logs"
  wandb:
    enabled: true
    project: "cmu-10799-flow_matching"
    entity: null
